from dotenv import load_dotenv
import tempfile
import os
import torch
from fastapi import FastAPI, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import List
# from llama_cloud_services import LlamaParse
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, get_response_synthesizer, Document, Settings
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.openai import OpenAI
from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow, AgentOutput, ToolCall, ToolCallResult
from llama_index.core.workflow import (
    step,   
    Context,
)
from llama_index.core.prompts import PromptTemplate

import shutil
import traceback
import nest_asyncio
nest_asyncio.apply()

# === Load environment ===
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("Missing OPENAI_API_KEY in environment or .env")

# === Fix torch bug if needed ===
torch.classes.__path__ = []
# llama_index.core.set_global_handler("simple")

# === FastAPI app setup ===
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# @app.middleware("http")
# async def log_requests(request, call_next):
#     print("Incoming request headers:", dict(request.headers))
#     response = await call_next(request)
#     return response

# === LLM + Parser ===
llm = OpenAI(api_key=OPENAI_API_KEY)
# parser = LlamaParse(
#     result_type="markdown",
#     parse_mode="parse_document_with_llm",
#     auto_mode_trigger_on_table_in_page=True,
#     auto_mode_trigger_on_image_in_page=True,
#     continuous_mode=True,
#     spreadsheet_extract_sub_tables=True
# )
# ====USING MARKITDOWN====
from markitdown import MarkItDown
md = MarkItDown(enable_plugins=False) # Set to True to enable plugins


# === Tools warehouse ===
async def search_documents(query: str) -> str:
    storage_context = StorageContext.from_defaults(persist_dir="storage")
    index = load_index_from_storage(storage_context)
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=5,
    )
    # configure response synthesizer
    response_synthesizer = get_response_synthesizer(
        response_mode="tree_summarize",
    )

    # assemble query engine
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=response_synthesizer,
    )
    response = await query_engine.aquery(query)
    return str(response)

#Our record_notes tool will access the current state, add the notes to the state,
#  and then return a message indicating that the notes have been recorded.
async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:
    """Useful for recording notes on a given topic."""
    current_state = await ctx.get("state")
    if "research_notes" not in current_state:
        current_state["research_notes"] = {}
    current_state["research_notes"][notes_title] = notes
    await ctx.set("state", current_state)
    return "Notes recorded. Handoff to StandardExpertAgent"

#write_report and review_report will similarly be tools that access the state:
async def write_report(ctx: Context, report_content: str) -> str:
    """Useful for writing a report on a given topic."""
    current_state = await ctx.get("state")
    current_state["report_content"] = report_content
    await ctx.set("state", current_state)
    return "Report written."


async def review_report(ctx: Context, review: str) -> str:
    """Useful for reviewing a report and providing feedback."""
    current_state = await ctx.get("state")
    current_state["review"] = review
    await ctx.set("state", current_state)
    return "Report reviewed."


# === Upload and store documents ===
@app.post("/uploadfiles/")
async def upload_files(files: List[UploadFile]):
    model = "text-embedding-3-large"
    Settings.embed_model = OpenAIEmbedding(model=model)
    print("Start working, embedding model:", model)
    try:
        file_paths = []

        for file in files:
            # Extract original file extension
            original_extension = os.path.splitext(file.filename)[1]
            
            # Create a temporary file with the original extension
            with tempfile.NamedTemporaryFile(delete=False, suffix=original_extension) as tmp:
                shutil.copyfileobj(file.file, tmp)
                tmp.flush()
                file_paths.append(tmp.name)
        print("file paths:", file_paths)
        # Parse using LlamaParse
        # documents = parser.load_data(file_paths)
        # Parse using Markitdown
        join_documents = [md.convert(file_path) for file_path in file_paths]
        documents_text = "\n".join([doc.text_content for doc in join_documents])
        print("Successfully parsed documents", documents_text)

        # Index and persist
        documents = [Document(text=doc.text_content) for doc in join_documents]
        index = VectorStoreIndex.from_documents(documents) #for llamaparse just feed documents is enough
        index.storage_context.persist(persist_dir="storage")

        # Optional: clean up files afterward
        for path in file_paths:
            os.remove(path)

        return {"message": "Inserted and stored files successfully"}

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"{type(e).__name__}: {e}")


#=== Query with uploaded files + existing index ===
@app.post("/query/")
async def query_with_context(files: List[UploadFile]):
    try:
        # Step 1: Parse new uploaded files
        file_paths = []

        for file in files:
            # Extract original file extension
            original_extension = os.path.splitext(file.filename)[1]
            
            # Create a temporary file with the original extension
            with tempfile.NamedTemporaryFile(delete=False, suffix=original_extension) as tmp:
                shutil.copyfileobj(file.file, tmp)
                tmp.flush()
                file_paths.append(tmp.name)
        print("file paths:", file_paths)
        # Parse using LlamaParse
        # new_documents = parser.load_data(file_paths)
        # Parse using Markitdown
        join_new_documents = [md.convert(file_path) for file_path in file_paths]
        new_documents = "\n".join([doc.text_content for doc in join_new_documents])
        # new_documents = "Diá»‡n tÃ­ch sÃ n xÃ¢y dá»±ng phá»¥c vá»¥ Ä‘Ã o táº¡o trÃªn sá»‘ ngÆ°á»i há»c chÃ­nh quy quy Ä‘á»•i theo trÃ¬nh Ä‘á»™ vÃ  lÄ©nh vá»±c Ä‘Ã o táº¡o cá»§a chÃºng tÃ´i lÃ  15 m2; 60% giáº£ng viÃªn toÃ n thá»i gian Ä‘Æ°á»£c bá»‘ trÃ­ chá»— lÃ m viá»‡c riÃªng biá»‡t."
        print("Successfully parsed documents: ", new_documents)

        

        # Step 4: Build the agents
        model = OpenAI(model="gpt-4o")
        record_agent = FunctionAgent(
            name="RecordAgent",
            description="Há»¯u Ã­ch trong viá»‡c nháº­n bÃ¡o cÃ¡o tÃ¬nh hÃ¬nh thá»±c táº¿ triá»ƒn khai cá»§a trÆ°á»ng Ä‘áº¡i há»c rá»“i tá»•ng há»£p thÃ nh ghi chÃº. After summarizing the report, hand off to StandardExpertAgent.",
            tools=[record_notes],
            llm=model,
            system_prompt=(
                "Báº¡n lÃ  cÃ¡n bá»™ tiáº¿p nháº­n bÃ¡o cÃ¡o tá»« cÃ¡c trÆ°á»ng Ä‘áº¡i há»c vá» "
                "tÃ¬nh hÃ¬nh triá»ƒn khai thá»±c táº¿ cá»§a há». "
                "CÃ´ng viá»‡c cá»§a báº¡n lÃ  Ä‘á»c bÃ¡o cÃ¡o rá»“i tá»•ng há»£p cÃ¡c thÃ´ng tin Ä‘Æ°á»£c Ä‘á» cáº­p trong Ä‘Ã³ thÃ nh "
                "Ä‘áº§y Ä‘á»§ cÃ¡c háº¡ng má»¥c Ä‘á»ƒ há»— trá»£ cÃ¡c cÃ¡n bá»™ khÃ¡c Ä‘Ã¡nh giÃ¡ tiÃªu chuáº©n sau nÃ y. "
                "Báº¡n pháº£i ghi rÃµ lÃ  báº¡n Ä‘Ã£ nháº­n bÃ¡o cÃ¡o gÃ¬ rá»“i chá»‰ cáº§n tá»•ng há»£p cÃ¡c háº¡ng má»¥c vÃ  khÃ´ng cáº§n pháº£i Ä‘Ã¡nh giÃ¡. "
                "LÆ°u Ã½ khÃ´ng Ä‘Æ°á»£c tá»± suy diá»…n mÃ  chá»‰ tá»•ng há»£p tá»« thÃ´ng tin Ä‘Æ°á»£c Ä‘á» cáº­p trong bÃ¡o cÃ¡o. "
                "Káº¿t quáº£ cÃ´ng viá»‡c cá»§a báº¡n lÃ  tÃ¡ch nhá» cÃ¡c háº¡ng má»¥c vÃ  Ä‘áº·t cÃ¢u há»i vá» quy Ä‘á»‹nh cá»§a háº¡ng má»¥c Ä‘Ã³ "
                "Sau khi tá»•ng há»£p xong hÃ£y bÃ n giao cho StandardExpertAgent"
                "If you have finished summarizing, respond with 'handoff to StandardExpertAgent'."
            ),
            can_handoff_to=["StandardExpertAgent"]
        )

        standard_expert = FunctionAgent(
            name="StandardExpertAgent",
            description="Há»¯u Ã­ch trong viá»‡c tÃ¬m kiáº¿m quy Ä‘á»‹nh liÃªn quan tá»›i cÃ¡c háº¡ng má»¥c Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c",
            tools=[search_documents],
            llm=model,
            system_prompt=(
                "Báº¡n lÃ  chuyÃªn gia vá» thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c "
                "CÃ´ng viá»‡c cá»§a báº¡n lÃ  tÃ¬m vÃ  trÃ­ch xuáº¥t quy Ä‘á»‹nh vá» thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c liÃªn quan tá»›i cÃ¡c háº¡ng má»¥c Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong tá»•ng há»£p. "
                "LÆ°u Ã½ khÃ´ng Ä‘Æ°á»£c tá»± suy diá»…n mÃ  pháº£i dÃ¹ng tool search_documents vÃ  dá»±a theo kiáº¿n thá»©c tá»« Ä‘Ã³ Ä‘á»ƒ trÃ­ch xuáº¥t. "
                "Báº¡n chá»‰ trÃ­ch xuáº¥t quy Ä‘á»‹nh cÃ³ liÃªn quan tá»›i cÃ¡c háº¡ng má»¥c Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong tá»•ng há»£p vÃ  khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ gÃ¬ vá» thá»±c táº¿ triá»ƒn khai á»Ÿ trÆ°á»ng Ä‘áº¡i há»c. "
                "VÃ¬ váº­y, trong káº¿t quáº£ tráº£ ra hÃ£y dáº«n nguá»“n rÃµ rÃ ng lÃ  tá»« Ä‘iá»u máº¥y, má»¥c nÃ o, hoáº·c theo tiÃªu chÃ­ sá»‘ máº¥y, thÃ´ng tÆ° nÃ o."
                "VÃ­ dá»¥: CÃC QUY Äá»ŠNH LIÃŠN QUAN Äáº¾N THá»°C Táº¾ TRIá»‚N KHAI:"
                "Háº¡ng má»¥c 1: Theo tiÃªu chÃ­ 2.1, thÃ´ng tÆ° 25/2015/TT-BYT, tá»· lá»‡ ngÆ°á»i há»c trÃªn giáº£ng viÃªn tá»‘i Ä‘a 40%"
                "Háº¡ng má»¥c 2: Theo tiÃªu chÃ­ 2.2, thÃ´ng tÆ° 25/2015/TT-BYT, tá»· lá»‡ giáº£ng viÃªn cÆ¡ há»¯u trong Ä‘á»™ tuá»•i lao Ä‘á»™ng tá»‘i thiá»ƒu 70%"
                "Khi trÃ­ch xuáº¥t xong, hÃ£y bÃ n giao káº¿t quáº£ cá»§a báº¡n cho QCOfficerAgent. "
                "If you have finished searching, respond with 'handoff to QCOfficerAgent'."
            ),
            can_handoff_to=["QCOfficerAgent"],
        )

        qc_officer = FunctionAgent(
            name="QCOfficerAgent",
            description="Há»¯u Ã­ch trong viá»‡c bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ tÃ¬nh hÃ¬nh thá»±c táº¿ triá»ƒn khai cá»§a trÆ°á»ng Ä‘áº¡i há»c",
            tools=[write_report,review_report],
            llm=model,
            system_prompt=(
                "Báº¡n lÃ  cÃ¡n bá»™ trÆ°á»Ÿng phÃ²ng kiá»ƒm Ä‘á»‹nh cháº¥t lÆ°á»£ng cá»§a Bá»™ GiÃ¡o Dá»¥c Viá»‡t Nam. "
                "CÃ´ng viá»‡c cá»§a báº¡n lÃ  bÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ theo dáº¡ng markdown vá» tÃ¬nh hÃ¬nh thá»±c táº¿ triá»ƒn khai cá»§a trÆ°á»ng Ä‘áº¡i há»c so sÃ¡nh, dá»±a theo thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c. "
                "Báº¡n sáº½ chá»‰ Ä‘áº¡o RecordAgent tá»•ng há»£p bÃ¡o cÃ¡o tÃ¬nh hÃ¬nh thá»±c táº¿ vÃ  StandardExpertAgent trÃ­ch xuáº¥t quy Ä‘á»‹nh. "
                "Báº¡n chá»‰ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ cÃ¡c háº¡ng má»¥c mÃ  RecordAgent Ä‘Ã£ tá»•ng há»£p, dá»±a theo thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c mÃ  StandardExpertAgent Ä‘Ã£ trÃ­ch xuáº¥t. "
                "BÃ¡o cÃ¡o cá»§a báº¡n pháº£i Ä‘i tuáº§n tá»± tá»«ng háº¡ng má»¥c, gá»“m thá»±c táº¿ Ä‘Ã£ triá»ƒn khai gÃ¬ (Thá»±c táº¿ triá»ƒn khai), "
                "quy Ä‘á»‹nh cá»§a thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c vá» háº¡ng má»¥c Ä‘Ã³ ra sao (Quy Ä‘á»‹nh) vÃ  "
                "thá»±c táº¿ triá»ƒn khai Ä‘Ã£ Ä‘áº¡t yÃªu cáº§u khÃ´ng (ÄÃ¡nh giÃ¡). "
                "VÃ­ dá»¥: ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG TRIá»‚N KHAI Cá»¦A TRÆ¯á»œNG Äáº I Há»ŒC A:"
                "Háº¡ng má»¥c 1: Tá»· lá»‡ ngÆ°á»i há»c trÃªn giáº£ng viÃªn lÃ  50."
                "Quy Ä‘á»‹nh: Tá»· lá»‡ ngÆ°á»i há»c trÃªn giáº£ng viÃªn lÃ  â‰¤ 40:1."
                "ÄÃ¡nh giÃ¡: KhÃ´ng Ä‘áº¡t, do tá»· lá»‡ ngÆ°á»i há»c trÃªn giáº£ng viÃªn lÃ  50, vÆ°á»£t quÃ¡ yÃªu cáº§u 40:1."
                "Háº¡ng má»¥c 2: Tá»· lá»‡ giáº£ng viÃªn cÆ¡ há»¯u trong Ä‘á»™ tuá»•i lao Ä‘á»™ng lÃ  15%."
                "Quy Ä‘á»‹nh: Tá»· lá»‡ giáº£ng viÃªn cÆ¡ há»¯u trong Ä‘á»™ tuá»•i lao Ä‘á»™ng lÃ  â‰¥ 70%."
                "ÄÃ¡nh giÃ¡: KhÃ´ng Ä‘áº¡t, do tá»· lá»‡ giáº£ng viÃªn cÆ¡ há»¯u trong Ä‘á»™ tuá»•i lao Ä‘á»™ng tháº¥p hÆ¡n quy Ä‘á»‹nh 70%."
                "Náº¿u báº¡n tháº¥y khÃ´ng Ä‘á»§ thÃ´ng tin hoáº·c khÃ´ng rÃµ, hÃ£y yÃªu cáº§u RecordAgent hoáº·c StandardExpertAgent cung cáº¥p thÃªm thÃ´ng tin."
            ),
            can_handoff_to=["RecordAgent", "StandardExpertAgent"],
        )

        # Step 5: Run the agent with new documents as context
        
        agent_workflow = AgentWorkflow(
            agents=[record_agent, standard_expert, qc_officer],
            root_agent=record_agent.name,
            initial_state={
                "research_notes": {},
                "report_content": "Not written yet.",
                "review": "Review required.",
                "original_user_request": f"""
                    ÄÃ¢y lÃ  bÃ¡o cÃ¡o vá» tÃ¬nh hÃ¬nh thá»±c táº¿ triá»ƒn khai cá»§a trÆ°á»ng Ä‘áº¡i há»c,
                    hÃ£y tá»•ng há»£p, phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ xem tá»«ng háº¡ng má»¥c thá»±c táº¿ triá»ƒn khai nÃ y Ä‘Ã£ Ä‘áº¡t yÃªu cáº§u theo
                    thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c chÆ°a: {new_documents}
                    Káº¿t quáº£ pháº£i lÃ  bÃ¡o cÃ¡o hoÃ n chá»‰nh, bao gá»“m:
                    - Thá»±c táº¿ triá»ƒn khai
                    - Quy Ä‘á»‹nh cá»§a thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c
                    - ÄÃ¡nh giÃ¡
                    """
            },
        )

        handler = agent_workflow.run(
            user_msg=f"""
            ÄÃ¢y lÃ  bÃ¡o cÃ¡o vá» tÃ¬nh hÃ¬nh thá»±c táº¿ triá»ƒn khai cá»§a trÆ°á»ng Ä‘áº¡i há»c,
            hÃ£y tá»•ng há»£p, phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ xem tá»«ng háº¡ng má»¥c thá»±c táº¿ triá»ƒn khai nÃ y Ä‘Ã£ Ä‘áº¡t yÃªu cáº§u theo
            thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c chÆ°a: {new_documents}
            Káº¿t quáº£ pháº£i lÃ  bÃ¡o cÃ¡o hoÃ n chá»‰nh dáº¡ng markdown, bao gá»“m:
            - Thá»±c táº¿ triá»ƒn khai
            - Quy Ä‘á»‹nh cá»§a thÃ´ng tÆ° ban hÃ nh Chuáº©n cÆ¡ sá»Ÿ giÃ¡o dá»¥c Ä‘áº¡i há»c
            - ÄÃ¡nh giÃ¡
            Pháº£i Ä‘áº£m báº£o táº¥t cáº£ agent Ä‘á»u hoáº¡t Ä‘á»™ng trong workflow.
            """
        )

        current_agent = None
        current_tool_calls = ""
        output_log = []
        async for event in handler.stream_events():
            if (
                hasattr(event, "current_agent_name")
                and event.current_agent_name != current_agent
            ):
                current_agent = event.current_agent_name
                print(f"\n{'='*50}")
                print(f"ğŸ¤– Agent: {current_agent}")
                print(f"{'='*50}\n")
                output_log.append(f"\n{'='*50}")
                output_log.append(f"ğŸ¤– Agent: {current_agent}")
                output_log.append(f"{'='*50}\n")
            elif isinstance(event, AgentOutput):
                if event.response.content:
                    print("ğŸ“¤ Output:", event.response.content)
                    output_log.append(f"ğŸ“¤ Output: {event.response.content}")
                if event.tool_calls:
                    print(
                        "ğŸ› ï¸  Planning to use tools:",
                        [call.tool_name for call in event.tool_calls],
                    )
                    output_log.append(f"ğŸ› ï¸  Planning to use tools: {', '.join([call.tool_name for call in event.tool_calls])}")
            elif isinstance(event, ToolCallResult):
                print(f"ğŸ”§ Tool Result ({event.tool_name}):")
                print(f"  Arguments: {event.tool_kwargs}")
                print(f"  Output: {event.tool_output}")
                output_log.append(f"ğŸ”§ Tool Result ({event.tool_name}):")
                output_log.append(f"  Arguments: {event.tool_kwargs}")
                output_log.append(f"  Output: {event.tool_output}")
            elif isinstance(event, ToolCall):
                print(f"ğŸ”¨ Calling Tool: {event.tool_name}")
                print(f"  With arguments: {event.tool_kwargs}")
                output_log.append(f"ğŸ”¨ Calling Tool: {event.tool_name}")
                output_log.append(f"  With arguments: {event.tool_kwargs}")
            # print(handler.state)
        return {"output_log": output_log}

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"{type(e).__name__}: {e}")

# === Run App ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
    
